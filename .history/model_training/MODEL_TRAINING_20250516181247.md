# Model Training Guide

This guide provides instructions for training new models for the App Longevity SaaS platform and how to integrate them into the application.

## Prerequisites

- Python 3.8+
- Training data (app store data with longevity metrics)
- Dependencies installed (scikit-learn, TensorFlow, PyTorch, etc.)

## Training Process

The training process consists of the following steps:

1. Data collection and preprocessing
2. Feature engineering
3. Model training and evaluation
4. Model export and integration

## Notebooks

The repository includes two Jupyter notebooks for model training:

- `app_longevity_training.ipynb`: Base model training notebook
- `app_longevity_training_new.ipynb`: Updated model training notebook with additional features

## Available Models

The system currently supports the following model types:

1. Random Forest (`rf_model`)
2. XGBoost (`xgboost_model`)
3. Neural Network (`neural_network_model`)
4. Combined Model (`combined_model`)

## Training a New Model

To train a new model:

1. Start with one of the existing notebooks as a template
2. Modify the hyperparameters or model architecture as needed
3. Train and evaluate the model
4. Export the model and required artifacts (see "Required Artifacts" below)

## Required Artifacts

For each model, the following artifacts need to be exported:

1. **Model File**: The trained model in a serialized format (`.joblib`, `.pkl`, `.h5`, or `.keras`)
2. **Scaler**: The fitted scaler used to preprocess the data (typically `scaler.joblib`)
3. **Preprocessor**: The fitted preprocessor if any additional preprocessing is performed (typically `preprocessor.pkl`)
4. **Feature Importances**: Feature importance scores in JSON format (`model_feature_importance.json`)
5. **Metadata**: Model metadata in JSON format (`model_metadata.json`)

### Example Metadata JSON:

```json
{
  "creation_date": "2023-08-01",
  "features_used": [
    "rating",
    "log_installs",
    "days_since_update",
    "app_age_days"
  ],
  "categorical_features": [
    "category",
    "store"
  ],
  "numerical_features": [
    "rating",
    "log_installs"
  ],
  "dataset_size": 1000,
  "best_model": {
    "name": "RandomForest",
    "test_rmse": 0.32,
    "test_r2": 0.95
  }
}
```

## Integrating a New Model into the App

After training, you can integrate your model into the app using one of the following methods:

### Method 1: Direct Replacement

1. Name your model file the same as the default model (e.g., `rf_model.joblib`)
2. Replace the existing model files in `backend/static/models/` with your new files
3. Restart the backend service

### Method 2: Adding a New Model

1. Save your model files with a unique name (e.g., `advanced_model.joblib`)
2. Copy the model files to `backend/static/models/` or another directory
3. Update the `DEFAULT_MODEL` environment variable if you want to make it the default

### Method 3: Using Environment Variables

1. Save your model files in any location
2. Set the `ADDITIONAL_MODEL_PATHS` environment variable to include your model directory
3. Set the `DEFAULT_MODEL` environment variable to your model's filename if desired

## Model Evaluation Guidelines

When evaluating a new model for production use, consider the following metrics:

1. **RMSE (Root Mean Squared Error)**: Lower is better
2. **RÂ² (Coefficient of Determination)**: Higher is better, closer to 1.0
3. **Feature Importance Analysis**: Verify feature contributions match domain knowledge
4. **Generalization**: Test on diverse app categories
5. **Performance**: Measure prediction time and resource requirements

## Example Export Code

```python
import joblib
import json
import os

# Export model
joblib.dump(model, 'advanced_model.joblib')

# Export scaler
joblib.dump(scaler, 'scaler.joblib')

# Export preprocessor
joblib.dump(preprocessor, 'preprocessor.pkl')

# Export feature importances
feature_importances = dict(zip(X_train.columns, model.feature_importances_))
with open('advanced_model_feature_importance.json', 'w') as f:
    json.dump(feature_importances, f, indent=2)

# Export metadata
metadata = {
    "creation_date": "2023-08-01",
    "features_used": X_train.columns.tolist(),
    "categorical_features": categorical_features,
    "numerical_features": numerical_features,
    "dataset_size": len(X_train),
    "best_model": {
        "name": "Advanced Model",
        "test_rmse": test_rmse,
        "test_r2": test_r2
    }
}

with open('advanced_model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
```

## Best Practices

1. Always export all required artifacts together
2. Include detailed metadata for traceability
3. Test your model with the actual backend code before deploying
4. Keep training notebooks and scripts in the `model_training/` directory
5. Document any changes to feature engineering or preprocessing
6. Version your models consistently 
